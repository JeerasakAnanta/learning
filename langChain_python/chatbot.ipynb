{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a Chatbot\n",
    "- ref : https://python.langchain.com/docs/tutorials/chatbot/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K\n",
      "added 35 packages in 6s\n",
      "\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K\n",
      "\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K8 packages are looking for funding\n",
      "\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K  run `npm fund` for details\n",
      "\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K"
     ]
    }
   ],
   "source": [
    "!pip install langchain-core langgraph>0.2.27 -q \n",
    "!npm i @langchain/ollama @langchain/core -g -q  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package              Version\n",
      "-------------------- -----------\n",
      "annotated-types      0.7.0\n",
      "anyio                4.8.0\n",
      "asttokens            3.0.0\n",
      "certifi              2025.1.31\n",
      "charset-normalizer   3.4.1\n",
      "comm                 0.2.2\n",
      "debugpy              1.8.12\n",
      "decorator            5.2.0\n",
      "executing            2.2.0\n",
      "h11                  0.14.0\n",
      "httpcore             1.0.7\n",
      "httpx                0.28.1\n",
      "idna                 3.10\n",
      "ipykernel            6.29.5\n",
      "ipython              8.32.0\n",
      "jedi                 0.19.2\n",
      "jsonpatch            1.33\n",
      "jsonpointer          3.0.0\n",
      "jupyter_client       8.6.3\n",
      "jupyter_core         5.7.2\n",
      "langchain-core       0.3.37\n",
      "langgraph            0.2.74\n",
      "langgraph-checkpoint 2.0.16\n",
      "langgraph-sdk        0.1.53\n",
      "langsmith            0.3.10\n",
      "matplotlib-inline    0.1.7\n",
      "msgpack              1.1.0\n",
      "nest-asyncio         1.6.0\n",
      "orjson               3.10.15\n",
      "packaging            24.2\n",
      "parso                0.8.4\n",
      "pexpect              4.9.0\n",
      "pip                  25.0.1\n",
      "platformdirs         4.3.6\n",
      "prompt_toolkit       3.0.50\n",
      "psutil               7.0.0\n",
      "ptyprocess           0.7.0\n",
      "pure_eval            0.2.3\n",
      "pydantic             2.10.6\n",
      "pydantic_core        2.27.2\n",
      "Pygments             2.19.1\n",
      "python-dateutil      2.9.0.post0\n",
      "PyYAML               6.0.2\n",
      "pyzmq                26.2.1\n",
      "requests             2.32.3\n",
      "requests-toolbelt    1.0.0\n",
      "six                  1.17.0\n",
      "sniffio              1.3.1\n",
      "stack-data           0.6.3\n",
      "tenacity             9.0.0\n",
      "tornado              6.4.2\n",
      "traitlets            5.14.3\n",
      "typing_extensions    4.12.2\n",
      "urllib3              2.3.0\n",
      "wcwidth              0.2.13\n",
      "zstandard            0.23.0\n"
     ]
    }
   ],
   "source": [
    "!pip list "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# instantiation\n",
    "- now we can instantiate our model  object and generate chat completions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU \"langchain[groq]\" -q "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install  dotenv -q  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dotenv \n",
    "dotenv.load_dotenv(\".env\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ[\"GROQ_API_KEY\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import  init_chat_model \n",
    "\n",
    "model = init_chat_model(\"llama3-8b-8192\", model_provider=\"groq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hi Bob! It's nice to meet you. Is there something I can help you with or would you like to chat?\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 26, 'prompt_tokens': 15, 'total_tokens': 41, 'completion_time': 0.021666667, 'prompt_time': 0.002291705, 'queue_time': 0.017737625, 'total_time': 0.023958372}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_a97cfe35ae', 'finish_reason': 'stop', 'logprobs': None}, id='run-c7099f4a-d2ab-4090-9fcb-20deb7c7e8f7-0', usage_metadata={'input_tokens': 15, 'output_tokens': 26, 'total_tokens': 41})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "model.invoke([HumanMessage(content=\"Hi! I'm Bob\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='I don\\'t have a personal name. I\\'m an artificial intelligence language model designed to assist and communicate with humans. I\\'m often referred to as \"Assistant\" or \"AI\" for short. I don\\'t have a personal identity or a physical presence, but I\\'m happy to help answer your questions and engage in conversation!', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 66, 'prompt_tokens': 15, 'total_tokens': 81, 'completion_time': 0.055, 'prompt_time': 0.003153147, 'queue_time': 0.020689402000000003, 'total_time': 0.058153147}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_179b0f92c9', 'finish_reason': 'stop', 'logprobs': None}, id='run-439dcd7e-d5c1-4b11-be51-0715838b7a76-0', usage_metadata={'input_tokens': 15, 'output_tokens': 66, 'total_tokens': 81})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke([HumanMessage(content=\"What is your name?\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Your name is Bob!', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 6, 'prompt_tokens': 40, 'total_tokens': 46, 'completion_time': 0.005, 'prompt_time': 0.007242221, 'queue_time': 0.019299209, 'total_time': 0.012242221}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_a97cfe35ae', 'finish_reason': 'stop', 'logprobs': None}, id='run-b9a805ed-3e99-41b7-8901-4c03b145ab3f-0', usage_metadata={'input_tokens': 40, 'output_tokens': 6, 'total_tokens': 46})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "model.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"Hi! I'm Bob\"),\n",
    "        AIMessage(content=\"Hello Bob! How can I assist you today?\"),\n",
    "        HumanMessage(content=\"What's my name?\"),\n",
    "    ]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
